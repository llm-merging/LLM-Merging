{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T05:16:27.831988Z","iopub.status.busy":"2024-09-06T05:16:27.831717Z","iopub.status.idle":"2024-09-06T05:16:29.713141Z","shell.execute_reply":"2024-09-06T05:16:29.712269Z","shell.execute_reply.started":"2024-09-06T05:16:27.831959Z"},"trusted":true},"outputs":[],"source":["## dataset.py\n","\n","import json \n","import pandas as pd\n","\n","class Dataset(object):\n","    def __init__(\n","        self,\n","        dataset_filepath: str,\n","    ):\n","        self.dataset = []\n","        self.dataset = pd.read_csv(dataset_filepath).to_dict('records')\n","        for dp in self.dataset:\n","            if not dp['answer_choices'] or dp['answer_choices'] != dp['answer_choices']:\n","                del dp['answer_choices']\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        return self.dataset[idx]\n","    \n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T05:16:29.715237Z","iopub.status.busy":"2024-09-06T05:16:29.714908Z","iopub.status.idle":"2024-09-06T05:16:49.648691Z","shell.execute_reply":"2024-09-06T05:16:49.647841Z","shell.execute_reply.started":"2024-09-06T05:16:29.715211Z"},"trusted":true},"outputs":[],"source":["## model/decoder_functions.py\n","\n","import torch\n","import torch.nn.functional as F\n","\n","def decoder_predict_multiple_choice(\n","    transformer, input_tokenizer, target_tokenizer, batch\n","): \n","    tokenized_batch = decoder_tokenize_batch(input_tokenizer, target_tokenizer, batch, transformer.device)\n","    output = transformer(\n","        input_ids=tokenized_batch[\"input_ids\"],\n","        attention_mask=tokenized_batch[\"input_mask\"],\n","        use_cache=True,\n","    )\n","    past_key_values = output.past_key_values\n","\n","    num_answer_choices = (\n","        tokenized_batch[\"answer_choices_ids\"].shape[0]\n","        // tokenized_batch[\"input_mask\"].shape[0]\n","    )\n","\n","    '''\n","    Expand the input_mask and past_key_values since these are the same and can be repeated for the different answer choices within an example \n","    '''\n","\n","    batch_size, max_input_len = tokenized_batch[\"input_mask\"].shape\n","    expanded_input_mask = torch.repeat_interleave(tokenized_batch[\"input_mask\"], num_answer_choices, dim=0)\n","\n","    expanded_past_key_valyes = []\n","    for pastKeyValues_perLayer in past_key_values:\n","        list_broadcast_pastKeyValues_perLayer = []\n","        for key_or_value in pastKeyValues_perLayer:\n","            # This is for keys or values which have dimension [batch_size, max_input_len, num_heads, head_dim]\n","            # This is the standard for Hugging Face.\n","            if len(key_or_value.shape) == 4:\n","                list_broadcast_pastKeyValues_perLayer.append(\n","                    torch.repeat_interleave(key_or_value, num_answer_choices, dim=0)\n","                )\n","            # This is for keys or values which have dimension [batch_size x num_heads, head_dim, max_input_len].\n","            # This is what is used for BLOOM in transformers == 4.22.0\n","            elif len(key_or_value.shape) == 3:\n","                num_heads = key_or_value.shape[0] // batch_size\n","                flatten_keyOrValue = key_or_value.reshape(\n","                    ((batch_size, num_heads) + key_or_value.shape[1:])\n","                )\n","                broadcast_flatten_keyOrValue = torch.repeat_interleave(\n","                    flatten_keyOrValue, num_answer_choices, dim=0\n","                )\n","                list_broadcast_pastKeyValues_perLayer.append(\n","                    broadcast_flatten_keyOrValue.flatten(0, 1)\n","                )\n","            else:\n","                raise ValueError(\n","                    f\"Invalid cached key or value shape: \", key_or_value.shape\n","                )\n","\n","        expanded_past_key_valyes.append(\n","            tuple(list_broadcast_pastKeyValues_perLayer)\n","        )\n","\n","\n","    # Combine the input mask and choice mask so the model knows which cached input representations\n","    # are padded when conditioning on the cached input representations.\n","    # [batch_size x num_choices, max_input_len + max_choice_len]\n","    combined_mask = torch.cat(\n","        [expanded_input_mask, tokenized_batch[\"answer_choices_mask\"]], dim=1\n","    )\n","\n","    # WARNING: The loss at transformer_outputs[0] is not valid, since allChoices_ids uses a\n","    # pad token of 0 and so the loss will not be ignored for the pad tokens\n","    transformer_outputs = transformer(\n","        input_ids=tokenized_batch[\"answer_choices_ids\"],\n","        attention_mask=combined_mask,\n","        past_key_values=expanded_past_key_valyes,\n","        use_cache=True,\n","    )\n","\n","    # We used the logits for all choices to compute the log probs per example since\n","    # the loss returned in transformer_outputs will average the negative log probs across examples\n","    # [batch_size x num_choices, max_choice_len, vocab_size]\n","    answer_choice_ids_logits = transformer_outputs.logits.float()\n","    vocab_size = answer_choice_ids_logits.shape[-1]\n","\n","    # Shift the ids, masks, and logits to handle predicting the next token for the decoder. Note that we need to pass in the input_ids and cannot rely on HuggingFace automatically constructing the ids from the labels, since we need to pass in an attention mask to handle the cached input representations.\n","    shifted_answer_choice_ids_logits = answer_choice_ids_logits[..., :-1, :].contiguous()\n","    shifted_answer_choice_ids = tokenized_batch[\"answer_choices_ids\"][\n","        ..., 1:\n","    ].contiguous()\n","    shifted_answer_choice_masks = tokenized_batch[\"answer_choices_mask\"][\n","        ..., 1:\n","    ].contiguous()\n","\n","    shifted_answer_choices_max_len = shifted_answer_choice_ids_logits.shape[1]\n","    vocab_size = shifted_answer_choice_ids_logits.shape[-1]\n","\n","    # Compute the log probability of the ids for all choices with respect to the logits [batch_size x num_choices x (max_choice_len-1)]\n","    shifted_answer_choice_ids_log_probs = -F.cross_entropy(\n","        shifted_answer_choice_ids_logits.view(-1, vocab_size),\n","        shifted_answer_choice_ids.view(-1),\n","        reduction=\"none\",\n","    )\n","\n","\n","    # [batch_size, num_answer_choices, answer_choices_max_len]\n","    shifted_answer_choice_ids_log_probs = shifted_answer_choice_ids_log_probs.reshape(\n","        -1, num_answer_choices, shifted_answer_choices_max_len\n","    )\n","\n","    shifted_answer_choices_mask = shifted_answer_choice_masks.reshape(\n","        -1, num_answer_choices, shifted_answer_choices_max_len\n","    )\n","\n","    answer_choice_log_probs = torch.sum(shifted_answer_choice_ids_log_probs * shifted_answer_choices_mask, dim=2)\n","\n","    _, predicted_choice = torch.max(answer_choice_log_probs, dim=1)\n","\n","    return predicted_choice, answer_choice_log_probs\n","\n","\n","\n","def decoder_generate(\n","    transformer, \n","    input_tokenizer,\n","    target_tokenizer,\n","    batch,\n","    max_gen_len\n","):\n","    tokenized_batch = decoder_tokenize_batch(input_tokenizer, target_tokenizer, batch, transformer.device)\n","\n","    generation_output = transformer.generate(\n","        input_ids=tokenized_batch[\"input_ids\"],\n","        attention_mask=tokenized_batch[\"input_mask\"],\n","        max_new_tokens=max_gen_len,\n","        eos_token_id=input_tokenizer.eos_token_id,\n","        pad_token_id=input_tokenizer.pad_token_id,\n","        bos_token_id=input_tokenizer.bos_token_id,\n","        do_sample=False,\n","        return_dict_in_generate=True,\n","    )\n","\n","    # Remove the original input ids from the generated ids to get just the generated ids \n","    input_len = tokenized_batch[f\"input_ids\"].shape[-1]\n","\n","    generated_ids = generation_output[\"sequences\"][:, input_len:]\n","\n","    generated_txt = input_tokenizer.batch_decode(\n","        generated_ids, skip_special_tokens=True\n","    )\n","\n","    return generation_output[\"sequences\"].cpu().numpy().tolist(), generated_txt\n","\n","def decoder_tokenize_batch(input_tokenizer, target_tokenizer, batch, device):\n","\n","    tokenized_batch = {}\n","\n","    keys_to_tokenize_with_tokenizer = [(\"input\", input_tokenizer), (\"answer_choices\", target_tokenizer), (\"target\", target_tokenizer)]\n","\n","\n","    # Tokenize keys which should be tokenized\n","    for key, tokenizer in keys_to_tokenize_with_tokenizer:\n","        if key in batch:\n","            # The values of the batch are normally a list of text.The exception is that for answer_choices, the values  is a list of list. We flatten this to a single list to pass is into the tokenizer \n","            if key == \"answer_choices\":\n","#                 print(batch[key])\n","                text = [item for list in batch[key] for item in list]\n","            else:\n","                text = batch[key]\n","\n","        tokenized_dict = tokenizer(\n","            text,\n","            return_tensors=\"pt\",\n","            padding=\"longest\",\n","            truncation=\"longest_first\",\n","        )\n","\n","        input_ids = tokenized_dict[\"input_ids\"]\n","        attention_mask = tokenized_dict[\"attention_mask\"]\n","\n","        if device is not None:\n","            input_ids = input_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","\n","        tokenized_batch[f\"{key}_ids\"] = input_ids\n","        tokenized_batch[f\"{key}_mask\"] = attention_mask\n","    return tokenized_batch"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T05:16:49.650057Z","iopub.status.busy":"2024-09-06T05:16:49.649735Z","iopub.status.idle":"2024-09-06T05:16:49.665882Z","shell.execute_reply":"2024-09-06T05:16:49.665154Z","shell.execute_reply.started":"2024-09-06T05:16:49.650032Z"},"trusted":true},"outputs":[],"source":["## model/encoder_decoder_functions.py\n","\n","import torch\n","import torch.nn.functional as F\n","\n","\n","def compute_loss(transformer, tokenizer, batch):\n","\n","    tokenized_batch = encoder_decoder_tokenize_batch(tokenizer, batch, transformer.device)\n","\n","    transformer_outputs = transformer(\n","        input_ids=tokenized_batch[\"input_ids\"],\n","        attention_mask=tokenized_batch[\"input_mask\"],\n","        labels=tokenized_batch[\"target_ids\"],\n","    )\n","\n","    # [batch_size, max_target_len, vocab_size]\n","    target_logits = transformer_outputs[1].float()\n","    vocab_size = target_logits.shape[-1]\n","\n","    # Compute the log probability of the ids for all choices with respect to the logits\n","    # [batch_size x max_target_len]\n","    negative_log_probs = F.cross_entropy(\n","        target_logits.reshape(-1, vocab_size),\n","        tokenized_batch[\"target_ids\"].reshape(-1),\n","        reduction=\"none\",\n","    )\n","\n","    # Zero out log_probs for target_ids with no loss\n","    target_mask = tokenized_batch[\"target_mask\"].reshape(-1)\n","    \n","    \n","    sum_negative_log_prob = torch.sum(\n","        negative_log_probs * target_mask\n","    )\n","\n","    loss = sum_negative_log_prob / torch.sum(\n","            target_mask\n","        )\n","\n","    return loss\n","\n","def encoder_decoder_predict_multiple_choice(\n","    transformer, tokenizer, batch\n","):\n","    tokenized_batch = encoder_decoder_tokenize_batch(tokenizer, batch, transformer.device)\n","#     print(tokenized_batch)\n","\n","    encoder_outputs = transformer.get_encoder()(\n","        tokenized_batch[\"input_ids\"],\n","        tokenized_batch[\"input_mask\"],\n","    )\n","\n","    # The answer_choices is the flattened batch of answer choices. To get the number of answer choices per example, we divide the total number of answer choices in a batch by the batch size. \n","    num_answer_choices = (\n","        tokenized_batch[\"answer_choices_ids\"].shape[0] // tokenized_batch[\"input_mask\"].shape[0]\n","    )\n","\n","    '''Expand the input_mask and encoder_outputs since these are the same and can be repeated for the different answer choices within an example \n","    '''\n","    # [batch_size x num_choices, max_input_len]\n","    expanded_input_mask = torch.repeat_interleave(tokenized_batch[\"input_mask\"], num_answer_choices, dim=0)\n","    # BaseModelOutput object from HuggingFace where the first element is the hidden states of the encoder at the last layer \n","    # [batch_size x num_choices, max_input_len, ff_dim]\n","    expanded_encoder_outputs = (\n","        torch.repeat_interleave(encoder_outputs[0], num_answer_choices, dim=0),\n","    )\n","\n","\n","    # WARNING: The loss at transformer_outputs[0] is not valid, since answer_choices_ids uses a pad token of 0 (while loss expects a pad token of -100) so the loss will not be ignored for the pad tokens. \n","    # The input mask is passed in for the cross encoder-decoder attention.\n","    transformer_outputs = transformer(\n","        attention_mask=expanded_input_mask,\n","        encoder_outputs=expanded_encoder_outputs,\n","        labels=tokenized_batch[\"answer_choices_ids\"],\n","    )\n","\n","    # We used the logits for all choices to compute the log probs per example since the loss returned in transformer_outputs will average the negative log probs across examples\n","    # [batch_size x num_choices, max_choice_len, vocab_size]\n","    answer_choice_ids_logits = transformer_outputs[1].float()\n","    answer_choices_max_len = answer_choice_ids_logits.shape[1]\n","    vocab_size = answer_choice_ids_logits.shape[-1]\n","\n","    # Compute the log probability of the ids for all choices with respect to the logits\n","    # [batch_size x num_choices x max_choice_len]\n","    answer_choices_ids_log_probs = -F.cross_entropy(\n","        answer_choice_ids_logits.view(-1, vocab_size),\n","        tokenized_batch[\"answer_choices_ids\"].view(-1),\n","        reduction=\"none\",\n","    )\n","\n","    # [batch_size, num_answer_choices, answer_choices_max_len]\n","    answer_choices_ids_log_probs = answer_choices_ids_log_probs.reshape(\n","        -1, num_answer_choices, answer_choices_max_len\n","    )\n","\n","    answer_choices_mask = tokenized_batch[\"answer_choices_mask\"].reshape(\n","        -1, num_answer_choices, answer_choices_max_len\n","    )\n","\n","    answer_choice_log_probs = torch.sum(answer_choices_ids_log_probs * answer_choices_mask, dim=2)\n","\n","    _, predicted_choice = torch.max(answer_choice_log_probs, dim=1)\n","\n","    return predicted_choice, answer_choice_log_probs\n","\n","\n","def encoder_decoder_generate(\n","    transformer, \n","    tokenizer,\n","    batch,\n","    max_gen_len\n","):\n","    tokenized_batch = encoder_decoder_tokenize_batch(tokenizer, batch, transformer.device)\n","\n","    generation_output = transformer.generate(\n","        input_ids=tokenized_batch[\"input_ids\"],\n","        attention_mask=tokenized_batch[\"input_mask\"],\n","        max_new_tokens=max_gen_len,\n","        eos_token_id=tokenizer.eos_token_id,\n","        pad_token_id=tokenizer.pad_token_id,\n","        bos_token_id=tokenizer.bos_token_id,\n","        do_sample=False,\n","        return_dict_in_generate=True,\n","    )\n","    generated_txt = tokenizer.batch_decode(\n","        generation_output[\"sequences\"], skip_special_tokens=True\n","    )\n","\n","    return generation_output[\"sequences\"].cpu().numpy().tolist(), generated_txt\n","\n","def encoder_decoder_tokenize_batch(tokenizer, batch, device):        \n","\n","    tokenized_batch = {}\n","\n","    # encoder decoder models pad to the right \n","    tokenizer.padding_side = \"right\"\n","\n","    keys_to_tokenize = [\"input\", \"answer_choices\", \"target\"]\n","\n","    for key in keys_to_tokenize:\n","        if key in batch:\n","            # The values of the batch are normally a list of text.The exception is that for answer_choices, the values  is a list of list. We flatten this to a single list to pass is into the tokenizer \n","            if key == \"answer_choices\":\n","#                 print(batch[key])\n","                text = [item for list in batch[key] for item in list]\n","            else:\n","                text = batch[key]\n","\n","            tokenized_dict = tokenizer(\n","                text,\n","                return_tensors=\"pt\",\n","                padding=\"longest\",\n","                truncation=\"longest_first\",\n","            )\n","\n","            input_ids = tokenized_dict[\"input_ids\"]\n","            attention_mask = tokenized_dict[\"attention_mask\"]\n","\n","            if device is not None:\n","                input_ids = input_ids.to(device)\n","                attention_mask = attention_mask.to(device)\n","\n","            tokenized_batch[f\"{key}_ids\"] = input_ids\n","            tokenized_batch[f\"{key}_mask\"] = attention_mask\n","\n","    return tokenized_batch\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T05:16:49.667324Z","iopub.status.busy":"2024-09-06T05:16:49.667087Z","iopub.status.idle":"2024-09-06T05:17:02.840873Z","shell.execute_reply":"2024-09-06T05:17:02.839555Z","shell.execute_reply.started":"2024-09-06T05:16:49.667301Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting peft\n","  Downloading peft-0.12.0-py3-none-any.whl (296 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/site-packages (from peft) (6.0.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from peft) (1.26.4)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/site-packages (from peft) (2.3.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from peft) (4.66.4)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/site-packages (from peft) (0.23.4)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (from peft) (4.42.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from peft) (6.0.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from peft) (24.1)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/site-packages (from peft) (0.4.3)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/site-packages (from peft) (0.32.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (8.9.2.26)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.0)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.3.1)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.4.5.107)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.0.106)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.1.105)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.0.2.54)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.20.5)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.2.106)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.5.82)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","Installing collected packages: peft\n","Successfully installed peft-0.12.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["## merges/Merges.py\n","# !pip install peft\n","import copy\n","import os\n","\n","from peft import load_peft_weights, PeftConfig\n","from safetensors.torch import save_file\n","\n","\n","from transformers import (\n","    AutoModelForSeq2SeqLM,\n","    AutoModelForCausalLM,\n","    AutoTokenizer\n",")\n","\n","class Merges(object):\n","\n","    def __init__(self, name):\n","        self.name = name\n","\n","        self.list_models = None\n","         \n","        self.loaded_models = None\n","        self.loaded_configs = None\n","        self.base_model = None\n","        self.tokenizer = None\n","        self.input_tokenizer = None\n","        self.target_tokenizer = None\n","\n","        self.base_model_name = None\n","        self.base_model_revision_id = None\n","\n","        self.max_seq_len = None\n","        self.max_gen_len = None\n","\n","        self.device = None\n","        self.architecture = None\n","\n","        self.merged_model = None\n","\n","    def get_name(self):\n","        return self.name\n","\n","    def get_model_config(self):\n","        raise NotImplementedError\n","\n","    def _load_base_model(self):\n","        if self.architecture == \"encoder_decoder\":\n","            self.base_model =  AutoModelForSeq2SeqLM.from_pretrained(self.base_model_name, revision=self.base_model_revision_id, token=os.environ[\"HF_AUTH_TOKEN\"]).to(self.device)\n","        elif self.architecture == \"decoder\":\n","            self.base_model =  AutoModelForCausalLM.from_pretrained(self.base_model_name, revision=self.base_model_revision_id, token=os.environ[\"HF_AUTH_TOKEN\"]).to(self.device)\n","        else:\n","            raise NotImplementedError(f\"Architecture not implemented {self.architecture}\")\n","        \n","\n","    def _load_tokenizer(self):\n","\n","        if self.architecture == \"encoder_decoder\":\n","            if self.tokenizer is None:\n","\n","                self.tokenizer = AutoTokenizer.from_pretrained(\n","                    self.base_model_name,\n","                    revision=self.base_model_revision_id,\n","                    model_max_length=self.max_seq_len,\n","                    legacy=False,\n","                    token=os.environ[\"HF_AUTH_TOKEN\"]\n","                )\n","\n","        elif self.architecture == \"decoder\":\n","            if self.input_tokenizer is None or self.target_tokenizer is None:\n","                    \n","                self.input_tokenizer = AutoTokenizer.from_pretrained(\n","                    self.base_model_name,\n","                    revision=self.base_model_revision_id,\n","                    model_max_length=self.max_seq_len,\n","                    legacy=False,\n","                    token=os.environ[\"HF_AUTH_TOKEN\"]\n","                )\n","                self.target_tokenizer = copy.deepcopy(self.input_tokenizer)\n","\n","                # Use eos_token for pad_token if it doesn't exist. This is ok since the\n","                # pad tokens will be ignored through the mask\n","                if self.input_tokenizer.pad_token_id is None:\n","                    self.input_tokenizer.pad_token_id = self.input_tokenizer.eos_token_id\n","                if self.target_tokenizer.pad_token_id is None:\n","                    self.target_tokenizer.pad_token_id = self.target_tokenizer.eos_token_id\n","\n","                # Add BOS and not EOS token \n","                self.input_tokenizer.padding_side = \"left\"\n","\n","                # Add EOS and not BOS token \n","                self.target_tokenizer.padding_side = \"right\"\n","                self.target_tokenizer.add_bos_token = False\n","                self.target_tokenizer.add_eos_token = True\n","        else:\n","            raise NotImplementedError(f\"Architecture not implemented {self.architecture}\")\n","\n","    def predict_multiple_choice(self, batch):\n","        assert self.base_model is not None\n","        if self.architecture == \"encoder_decoder\":\n","            assert self.tokenizer is not None\n","            return encoder_decoder_predict_multiple_choice(self.base_model, self.tokenizer, batch)\n","        elif self.architecture == \"decoder\":\n","            return decoder_predict_multiple_choice(self.base_model, self.input_tokenizer, self.target_tokenizer, batch)\n","        else:\n","            raise NotImplementedError(f\"Architecture not implemented {self.architecture}\")\n","    \n","    def generate(self, batch):\n","        assert self.base_model is not None\n","        if self.architecture == \"encoder_decoder\":\n","            assert self.tokenizer is not None\n","            return encoder_decoder_generate(self.base_model, self.tokenizer, batch, self.max_gen_len)\n","        elif self.architecture == \"decoder\":\n","            return decoder_generate(self.base_model, self.input_tokenizer, self.target_tokenizer, batch, self.max_gen_len)\n","        else:\n","            raise NotImplementedError(f\"Architecture not implemented {self.architecture}\")\n","\n","    def _load_huggingface_models_and_configs(self):\n","        assert len(self.list_models) > 0, f\"List of models must include at leat 1 model\"\n","\n","        parameter_names = None\n","        for model_name, revision_id in self.list_models:\n","\n","            peft_model_parameters = load_peft_weights(model_name, revision=revision_id, token=os.environ[\"HF_AUTH_TOKEN\"])\n","            peft_config = PeftConfig.from_pretrained(model_name)\n","\n","            if parameter_names is None:\n","                parameter_names = set(peft_model_parameters.keys())\n","\n","            if parameter_names != set(peft_model_parameters.keys()):\n","                print(f\"WARNING: parameters in {model_name} do not match {self.list_models[0]}\")\n","\n","            self.loaded_models[model_name] = peft_model_parameters \n","            self.loaded_configs[model_name] = peft_config\n","\n","    def merge(\n","        self,\n","    ):\n","        raise NotImplementedError\n","    \n","    def save_model(self, output_dir):\n","        assert self.merged_model is not None, \"Merged model is empty\"\n","        assert len(self.merged_model) > 0, \"Merged model is empty\"\n","        # Save merged model as safetensor \n","        save_file(self.merged_model, os.path.join(output_dir, \"safetensor.pt\"))"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T05:17:02.845018Z","iopub.status.busy":"2024-09-06T05:17:02.844237Z","iopub.status.idle":"2024-09-06T05:17:02.855950Z","shell.execute_reply":"2024-09-06T05:17:02.855336Z","shell.execute_reply.started":"2024-09-06T05:17:02.844976Z"},"trusted":true},"outputs":[],"source":["## merges/LlamaAvg.py\n","\n","import torch \n","\n","from peft import get_peft_model, set_peft_model_state_dict\n","\n","class LlamaAvg(Merges):\n","    def __init__(self, name):\n","        super().__init__(name)\n","\n","        '''\n","        These values are meant to be modified by the user.\n","        '''\n","        # Give a list of models to load for the merge. Each element is the list a is a tuple of (model, revision_id). We recommend specifying a revision id to ensure the model was not modified after May 31 \n","        self.list_models = [(\"abcdabcd987/gsm8k-llama2-7b-lora-16\", \"636b5eb8da724edae406ba69ef90fd06478e6df7\"), \n","                            (\"FinGPT/fingpt-forecaster_dow30_llama2-7b_lora\", \"69f77190315afdb03a889d89bf2a0f932b311617\")]\n","\n","        # Hyperparameters \n","        self.base_model_name = \"meta-llama/Llama-2-7b-hf\"\n","        # We recommend specifying a revision id to ensure the model was not modified after May 31 \n","        self.base_model_revision_id = \"01c7f73d771dfac7d292323805ebc428287df4f9\"\n","\n","\n","        self.max_seq_len = None\n","        self.max_gen_len = 64\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        # Architecture must match base model. \n","        self.architecture = \"decoder\"\n","        '''\n","        These are variables used later in the code and not intended to be set, but feel free to adapt to your use case.  \n","        '''\n","        # Loaded models and configs \n","        self.loaded_models = {}\n","        self.loaded_configs = {}\n","\n","        # Merged model parameters\n","        self.merged_model = {}\n","\n","\n","\n","    # Implement merge function \n","    def merge(\n","        self,\n","    ):\n","\n","        '''\n","        1) Load HuggingFace checkpoints and configs \n","        '''\n","        super()._load_huggingface_models_and_configs()\n","        '''\n","        2) Merge checkpoints  \n","        '''\n","        parameter_lambdas = [0.8, 0.2]\n","\n","        # Get individual models \n","        all_models = list(self.loaded_models.values())\n","\n","        # Get all the parameters names (uses the first model and assume all the models have the same parameter)\n","        all_parameter_names = all_models[0].keys()\n","\n","        for parameter_name in all_parameter_names:\n","            merged_parameter = None\n","            for parameter_lambda, model in zip(parameter_lambdas, all_models):\n","                parameter = model[parameter_name]\n","                if merged_parameter is None:\n","                    merged_parameter = torch.clone(parameter) * parameter_lambda\n","                else:\n","                    # first model has rank 16 and second model has rank 8, so we expand the second model to rank 16 by adding zeros\n","                    if \"A\" in parameter_name:\n","                        parameter = torch.cat([torch.zeros_like(parameter), parameter], dim=0)\n","                    else:\n","                        assert \"B\" in parameter_name\n","                        parameter = torch.cat([torch.zeros_like(parameter), parameter], dim=1)\n","                    merged_parameter += parameter * parameter_lambda\n","            self.merged_model[parameter_name] = merged_parameter\n","\n","        '''\n","        3) Load base model and tokenizer\n","        '''\n","        self._load_base_model()\n","        self._load_tokenizer()\n","\n","        '''\n","        4) Load merged model into base model \n","        '''\n","        # Modify the base model. This is needed for Peft, which wraps the base_model in a Peft wrapper. \n","        huggingface_config = list(self.loaded_configs.values())[0]\n","        if huggingface_config is not None:\n","            self.base_model = get_peft_model(self.base_model, huggingface_config)\n","            set_peft_model_state_dict(self.base_model, self.merged_model)\n","        \n","        else:\n","            self.base_model.load(self.merged_model)\n","\n","        # Requires to make results deterministic. If not set, we will just run once and use the results from the first pass. \n","        self.base_model.eval()\n","\n","        return self.base_model"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T05:17:02.857031Z","iopub.status.busy":"2024-09-06T05:17:02.856812Z","iopub.status.idle":"2024-09-06T05:17:02.873815Z","shell.execute_reply":"2024-09-06T05:17:02.873122Z","shell.execute_reply.started":"2024-09-06T05:17:02.857009Z"},"trusted":true},"outputs":[],"source":["## merges/FlanT5Avg.py\n","\n","import torch \n","\n","from peft import get_peft_model, set_peft_model_state_dict\n","\n","class FlanT5Avg(Merges):\n","    def __init__(self, name):\n","        super().__init__(name)\n","\n","\n","        '''\n","        These values are meant to be modified by the user.\n","        '''\n","        # Give a list of models to load for the merge \n","        self.list_models = [(\"lorahub/flan_t5_xl-wiki_qa_Is_This_True_\", \"30a1ee2f857196c1eb996d854548cc19f45ac642\"), \n","                            (\"lorahub/flan_t5_xl-kilt_tasks_hotpotqa_complex_question\", \"27d014366bec1c5333ba2e2fae966b7de3c02df1\")]\n","        \n","        # Hyperparameters \n","        self.base_model_name = \"google/flan-t5-xl\"\n","        self.base_model_revision_id = \"7d6315df2c2fb742f0f5b556879d730926ca9001\"\n","        self.max_seq_len = 512\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        # Architecture must match base model. \n","        self.architecture = \"encoder_decoder\"\n","\n","        '''\n","        These are variables used later in the code and not intended to be set, but feel free to adapt to your use case.  \n","        '''\n","        # Loaded models and configs \n","        self.loaded_models = {}\n","        self.loaded_configs = {}\n","\n","        # Merged model parameters\n","        self.merged_model = {}\n","\n","    # Implement merge function \n","    def merge(\n","        self,\n","    ):\n","\n","        '''\n","        1) Load HuggingFace checkpoints and configs \n","        '''\n","        super()._load_huggingface_models_and_configs()\n","\n","        '''\n","        2) Merge checkpoints  \n","        '''\n","        parameter_lambdas = [0.5, 0.5]\n","\n","        # Get individual models \n","        all_models = list(self.loaded_models.values())\n","\n","        # Get all the parameters names (uses the first model and assume all the models have the same parameter)\n","        all_parameter_names = all_models[0].keys()\n","\n","        for parameter_name in all_parameter_names:\n","            merged_parameter = None\n","            for parameter_lambda, model in zip(parameter_lambdas, all_models):\n","                parameter = model[parameter_name]\n","                if merged_parameter is None:\n","                    merged_parameter = torch.clone(parameter) * parameter_lambda\n","                else:\n","                    merged_parameter += parameter * parameter_lambda\n","            self.merged_model[parameter_name] = merged_parameter\n","        '''\n","        3) Load base model and tokenizer \n","        '''\n","        self._load_base_model()\n","        self._load_tokenizer()\n","\n","        '''\n","        4) Load merged model into base model \n","        '''\n","        # Modify the base model. This is needed for Peft, which wraps the base_model in a Peft wrapper. \n","        huggingface_config = list(self.loaded_configs.values())[0]\n","        if huggingface_config is not None:\n","            self.base_model = get_peft_model(self.base_model, huggingface_config)\n","            set_peft_model_state_dict(self.base_model, self.merged_model)\n","        \n","        else:\n","            self.base_model.load(self.merged_model)\n","\n","        # Requires to make results deterministic. If not set, we will just run once and use the results from the first pass. \n","        self.base_model.eval()\n","\n","        return self.base_model"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T05:17:22.268205Z","iopub.status.busy":"2024-09-06T05:17:22.267814Z","iopub.status.idle":"2024-09-06T05:17:42.803668Z","shell.execute_reply":"2024-09-06T05:17:42.802847Z","shell.execute_reply.started":"2024-09-06T05:17:22.268177Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting evaluate\n","  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/site-packages (from evaluate) (4.66.4)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/site-packages (from evaluate) (3.5.0)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/site-packages (from evaluate) (2.21.0)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/site-packages (from evaluate) (2024.6.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/site-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/site-packages (from evaluate) (0.23.4)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/site-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from evaluate) (1.26.4)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/site-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from evaluate) (24.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.4)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.10.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.2.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.11)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","Installing collected packages: evaluate\n","Successfully installed evaluate-0.4.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: Logging before InitGoogle() is written to STDERR\n","E0000 00:00:1725599853.674774      13 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n","=== Source Location Trace: ===\n","learning/45eac/tfrc/runtime/common_lib.cc:479\n","E0906 05:17:33.706609309     256 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:\"2024-09-06T05:17:33.706593012+00:00\"}\n"]}],"source":["## evaluate.py\n","# !pip install evaluate\n","\n","import evaluate \n","import json \n","import os \n","import pandas as pd\n","\n","from typing import List, Dict, Any\n","\n","import torch\n","from tqdm import tqdm\n","from torch.utils import data\n","\n","\n","def convert_dict_of_lists_to_list_of_dicts(dict_of_lists: Dict[Any, List]) -> List[Dict]:\n","    \"\"\"\n","    Args:\n","        dict_of_lists:\n","\n","    Returns:\n","        list_ofDict\n","    \"\"\"\n","    list_of_dicts = []\n","    for datapoint_values in zip(*dict_of_lists.values()):\n","        list_of_dicts.append(dict(zip(dict_of_lists, datapoint_values)))\n","    return list_of_dicts\n","\n","def collate_fn(batch_of_datapoints: List[Dict]) -> Dict[Any, List]:\n","    \"\"\"\n","    Convert a batch of datapoints into a datapoint that is batched. This is meant to override the default collate function in pytorch and specifically can handle when the value is a list \n","\n","    Args:\n","        batch_ofDatapoints:\n","\n","    Returns:\n","\n","    \"\"\"\n","    datapoint_batched = {}\n","    for datapoint in batch_of_datapoints:\n","        # Gather together all the values per key\n","        for key, value in datapoint.items():\n","            if key in datapoint_batched:\n","                datapoint_batched[key].append(value)\n","            else:\n","                datapoint_batched[key] = [value]\n","    return datapoint_batched\n","\n","\n","def evaluate_dataset(\n","    merge_method,\n","    dataset_filepath: str,\n",") -> (Dict, List):\n","\n","    data_loader = data.DataLoader(\n","        Dataset(dataset_filepath),\n","        batch_size=1,\n","        num_workers=0,\n","        shuffle=False,\n","        collate_fn=collate_fn\n","    )\n","\n","    all_batches = []\n","\n","    with torch.no_grad():\n","        for batch in tqdm(data_loader):\n","            # There are two types of evaluation models:\n","            # 1) multiple choice where the model scores each choice and predicts the choice with the highest score \n","            # 2) generation where the model generate some output give some input \n","            eval_type = batch[\"eval_type\"][0]\n","            \n","            if eval_type == \"multiple_choice\":\n","                (\n","                    predicted_choice,\n","                    answer_choice_scores,\n","                ) = merge_method.predict_multiple_choice(batch)\n","\n","                batch[\"prediction\"] = str(predicted_choice.cpu().numpy().tolist()[0])\n","                all_batches.extend(convert_dict_of_lists_to_list_of_dicts(batch))\n","            \n","            else:\n","                assert eval_type == \"generation\"\n","                (\n","                    generated_ids, generated_txt\n","                ) = merge_method.generate(batch\n","                )\n","                batch[\"prediction\"] = generated_txt \n","                all_batches.extend(convert_dict_of_lists_to_list_of_dicts(batch))\n","\n","    return all_batches\n","\n","def evaluate_model(\n","    merge_method,\n","    all_dataset_filepaths: List[str],\n",") -> Dict:   \n","    output_dir = os.path.join(\"output\", merge_method.get_name())\n","    prediction_dir = os.path.join(output_dir, \"predictions\")\n","    os.makedirs(prediction_dir, exist_ok=True)\n","    # Save merged model \n","    merge_method.save_model(output_dir)\n","\n","    all_scores = {}\n","\n","    for dataset_filepath in all_dataset_filepaths:\n","        dataset_predictions = evaluate_dataset(merge_method, dataset_filepath)\n","        dp_df = pd.DataFrame(dataset_predictions)\n","        dp_df[\"dummy_field\"] = 0\n","        dp_df.to_csv(\"../submission.csv\", columns=[\"id\", \"prediction\", \"dummy_field\"], index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-06T05:17:42.805213Z","iopub.status.busy":"2024-09-06T05:17:42.804690Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading shards: 100%|██████████| 2/2 [01:03<00:00, 31.62s/it]\n","Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]\n","  0%|          | 0/807 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","  0%|          | 2/807 [02:00<13:33:32, 60.64s/it]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n","  3%|▎         | 26/807 [14:28<8:01:30, 36.99s/it] "]}],"source":["## main.py\n","\n","def all_merge_handlers():\n","    \"\"\"Enumerate and Load (import) all merge methods.\"\"\"\n","    loaded_merges = {\n","        \"llama_avg\": LlamaAvg,\n","        \"tiny_llama_avg\": TinyLlamaAvg,\n","        \"flant5_avg\": FlanT5Avg,\n","    }\n","    \n","    \n","    return loaded_merges\n","\n","# Load correct merging method \n","merging_method = \"LlamaAvg\"\n","dataset_filepaths = [\"../data/validation.csv\"]\n","os.environ[\"HF_AUTH_TOKEN\"] = \"\" # TODO\n","loaded_merges = all_merge_handlers()\n","merge_method = loaded_merges[merging_method](merging_method)\n","\n","# Call the merge function. The merged model is stored under merging_method object \n","merge_method.merge()\n","\n","# Evaluate method on datsets passed in (used for testing)\n","evaluate_model(\n","    merge_method,\n","    dataset_filepaths,\n",")\n","\n"]}],"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"databundleVersionId":9317659,"sourceId":82622,"sourceType":"competition"}],"dockerImageVersionId":30746,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
